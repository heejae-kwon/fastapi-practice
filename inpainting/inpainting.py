# -*- coding: utf-8 -*-
"""in-painting with stable diffusion using ðŸ§¨diffusers

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/in_painting_with_stable_diffusion_using_diffusers.ipynb

# In-painting pipeline for Stable Diffusion using ðŸ§¨ Diffusers

This notebook shows how to do text-guided in-painting with Stable Diffusion model using  ðŸ¤— Hugging Face [ðŸ§¨ Diffusers library](https://github.com/huggingface/diffusers).

For a general introduction to the Stable Diffusion model please refer to this [colab](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb).
"""

#!pip install -qq -U diffusers==0.11.1 transformers ftfy gradio accelerate

"""To use private and gated models on ðŸ¤— Hugging Face Hub, login is required. If you are only using a public checkpoint (such as `runwayml/stable-diffusion-inpainting` in this notebook), you can skip this step."""

# from huggingface_hub import notebook_login

# notebook_login()


from io import BytesIO
from pathlib import Path
import requests
import torch
import PIL
from PIL import Image
from diffusers import StableDiffusionInpaintPipeline
device = "cuda"
model_path = "runwayml/stable-diffusion-inpainting"

pipe = StableDiffusionInpaintPipeline.from_pretrained(
    model_path,
    torch_dtype=torch.float16,
).to(device)


def image_grid(imgs, rows, cols) -> Image.Image:

    assert len(imgs) == rows*cols

    w, h = imgs[0].size
    grid: Image.Image = PIL.Image.new('RGB', size=(cols*w, rows*h))
    grid_w, grid_h = grid.size

    for i, img in enumerate(imgs):
        grid.paste(img, box=(i % cols*w, i//cols*h))
    return grid


def download_image(url):
    response = requests.get(url)
    return PIL.Image.open(BytesIO(response.content)).convert("RGB")


def run_in_painting_with_stable_diffusion(img_path: Path,
                                          mask_path: Path,
                                          result_path: Path,
                                          prompt: str):
    # img_url = "https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png"
    # mask_url = "https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png"

    image = Image.open(img_path).convert("RGB")
    image = image.resize((512, 512))

    mask_image = Image.open(mask_path).convert("RGB")
    mask_image = mask_image.resize((512, 512))

    # mask_image = download_image(mask_url).resize((512, 512))
    # mask_image

    # prompt = "a mecha robot sitting on a bench"

    guidance_scale = 7.5
    num_samples = 3
    generator = torch.Generator(device="cuda").manual_seed(
        0)  # change the seed to get different results

    images = pipe(
        prompt=prompt,
        image=image,
        mask_image=mask_image,
        guidance_scale=guidance_scale,
        generator=generator,
        num_images_per_prompt=num_samples,
    ).images

    # insert initial image in the list so we can compare side by side
    images.insert(0, image)

    result_img = image_grid(images, 1, num_samples + 1)
    result_img.save((result_path), "PNG")


"""### Gradio Demo"""
"""
gr.Interface(
    predict,
    title='Stable Diffusion In-Painting',
    inputs=[
        gr.Image(source='upload', tool='sketch', type='pil'),
        gr.Textbox(label='prompt')
    ],
    outputs=[
        gr.Image()
    ]
).launch(debug=True)
"""

if __name__ == '__main__':
    run_in_painting_with_stable_diffusion()
